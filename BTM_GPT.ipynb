{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc513231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading CSV: C:\\Users\\madhu\\Desktop\\ml_lab\\AML_lab_papers\\AML_lab_papers\\BTM\\archive(1)\\tweets.csv\n",
      "num of docs: 52542\n",
      "vocab: 8298\n",
      "num of biterms: 1333785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BTM (GPU):   1%|          | 4/500 [14:57<31:39:15, 229.75s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure NLTK components exist\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Preprocessing\n",
    "# -------------------------------------------------\n",
    "def preprocess_docs(docs, min_df=10):\n",
    "    tokenized = []\n",
    "    for d in docs:\n",
    "        if not isinstance(d, str):\n",
    "            tokenized.append([])\n",
    "            continue\n",
    "        d = d.lower()\n",
    "        tokens = word_tokenize(d)\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in STOPWORDS]\n",
    "        tokenized.append(tokens)\n",
    "\n",
    "    freq = Counter([w for doc in tokenized for w in doc])\n",
    "    vocab = {w for w, c in freq.items() if c >= min_df}\n",
    "    tokenized = [[w for w in doc if w in vocab] for doc in tokenized]\n",
    "    return tokenized, vocab\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Biterm extraction\n",
    "# -------------------------------------------------\n",
    "def extract_biterms(tokenized_docs, window=None):\n",
    "    biterms = []\n",
    "    doc_bit_indices = []\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        L = len(doc)\n",
    "        indices = []\n",
    "        for i in range(L):\n",
    "            jmax = L if window is None else min(L, i + window)\n",
    "            for j in range(i + 1, jmax):\n",
    "                if doc[i] == doc[j]:\n",
    "                    continue\n",
    "                wi, wj = sorted([doc[i], doc[j]])\n",
    "                indices.append(len(biterms))\n",
    "                biterms.append((wi, wj))\n",
    "        doc_bit_indices.append(indices)\n",
    "\n",
    "    return biterms, doc_bit_indices\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Vocabulary mapping\n",
    "# -------------------------------------------------\n",
    "def build_vocab_maps(vocab):\n",
    "    words = sorted(list(vocab))\n",
    "    w2id = {w: i for i, w in enumerate(words)}\n",
    "    id2w = {i: w for w, i in w2id.items()}\n",
    "    return w2id, id2w\n",
    "\n",
    "\n",
    "def encode_biterms(biterms, w2id):\n",
    "    return [(w2id[a], w2id[b]) for (a, b) in biterms]\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# GPU-Accelerated BTM with PyTorch\n",
    "# -------------------------------------------------\n",
    "class BTM_GPU:\n",
    "    def __init__(self, K=20, alpha=None, beta=0.01, iterations=500):\n",
    "        self.K = K\n",
    "        self.alpha = alpha if alpha is not None else 50.0 / K\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def fit(self, biterms, vocab_size, doc_biterms):\n",
    "        B = len(biterms)\n",
    "        M = vocab_size\n",
    "        K = self.K\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "\n",
    "        # Convert biterms to tensors on GPU\n",
    "        wi_tensor = torch.tensor([b[0] for b in biterms], device=device, dtype=torch.long)\n",
    "        wj_tensor = torch.tensor([b[1] for b in biterms], device=device, dtype=torch.long)\n",
    "\n",
    "        # Initialize counts (move to GPU)\n",
    "        n_z = torch.zeros(K, device=device)\n",
    "        n_wz = torch.zeros((M, K), device=device)\n",
    "        z_b = torch.randint(0, K, (B,), device=device)\n",
    "\n",
    "        # Initialize count matrices\n",
    "        for i in range(B):\n",
    "            z = z_b[i]\n",
    "            n_z[z] += 1\n",
    "            n_wz[wi_tensor[i], z] += 1\n",
    "            n_wz[wj_tensor[i], z] += 1\n",
    "\n",
    "        # Gibbs Sampling\n",
    "        for it in tqdm(range(self.iterations), desc=\"Training BTM (GPU)\"):\n",
    "            denom = n_wz.sum(0) + M * beta\n",
    "\n",
    "            for i in range(B):\n",
    "                zi = z_b[i]\n",
    "                wi = wi_tensor[i]\n",
    "                wj = wj_tensor[i]\n",
    "\n",
    "                # Remove current assignment\n",
    "                n_z[zi] -= 1\n",
    "                n_wz[wi, zi] -= 1\n",
    "                n_wz[wj, zi] -= 1\n",
    "\n",
    "                # Compute probabilities on GPU\n",
    "                pz = (n_z + alpha) \\\n",
    "                    * (n_wz[wi] + beta) / denom \\\n",
    "                    * (n_wz[wj] + beta) / denom\n",
    "\n",
    "                # Normalize\n",
    "                pz = pz / pz.sum()\n",
    "\n",
    "                # Sample new topic using PyTorch\n",
    "                new_z = Categorical(pz).sample()\n",
    "\n",
    "                # Add counts back\n",
    "                z_b[i] = new_z\n",
    "                n_z[new_z] += 1\n",
    "                n_wz[wi, new_z] += 1\n",
    "                n_wz[wj, new_z] += 1\n",
    "\n",
    "        # Compute phi & theta\n",
    "        self.phi = (n_wz + beta) / (n_wz.sum(0)[None, :] + M * beta)\n",
    "        self.theta = (n_z + alpha) / (B + K * alpha)\n",
    "\n",
    "        self.biterms = biterms\n",
    "        self.doc_biterms = doc_biterms\n",
    "        self.wi_tensor = wi_tensor\n",
    "        self.wj_tensor = wj_tensor\n",
    "        self.z_b = z_b\n",
    "        self.vocab_size = M\n",
    "\n",
    "    def infer_doc_topics(self, biterm_indices):\n",
    "        if len(biterm_indices) == 0:\n",
    "            return torch.ones(self.K, device=device) / self.K\n",
    "\n",
    "        z_dist = torch.zeros(self.K, device=device)\n",
    "        for b in biterm_indices:\n",
    "            wi = self.wi_tensor[b]\n",
    "            wj = self.wj_tensor[b]\n",
    "\n",
    "            pz = self.theta * (self.phi[wi] * self.phi[wj])\n",
    "            pz = pz / pz.sum()\n",
    "\n",
    "            z_dist += pz\n",
    "\n",
    "        return (z_dist / len(biterm_indices)).detach().cpu().numpy()\n",
    "\n",
    "    def top_words(self, id2w, top_n=10):\n",
    "        topics = []\n",
    "        phi_cpu = self.phi.cpu().numpy()\n",
    "        for k in range(self.K):\n",
    "            top_idx = phi_cpu[:, k].argsort()[::-1][:top_n]\n",
    "            topics.append([id2w[i] for i in top_idx])\n",
    "        return topics\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# MAIN PROGRAM (NO ARGUMENTS)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Your local dataset path (edit as needed)\n",
    "    \n",
    "    \n",
    "    csv_path = os.path.join(os.environ['USERPROFILE'], 'Desktop', 'ml_lab', 'AML_lab_papers', 'AML_lab_papers', 'BTM', 'archive(1)', 'tweets.csv')\n",
    "\n",
    "    print(\"Loading CSV:\", csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    text_col = \"content\"   # <--- change to your actual column\n",
    "    documents = df[text_col].astype(str).tolist()\n",
    "    print(f\"num of docs: {len(documents)}\")\n",
    "\n",
    "    # Preprocess\n",
    "    tokenized, vocab = preprocess_docs(documents, min_df=5)\n",
    "    print(f\"vocab: {len(vocab)}\")\n",
    "\n",
    "    # Biterms\n",
    "    biterms_raw, doc_biterms = extract_biterms(tokenized, window=None)\n",
    "\n",
    "    # Vocab\n",
    "    w2id, id2w = build_vocab_maps(vocab)\n",
    "    encoded_biterms = encode_biterms(biterms_raw, w2id)\n",
    "    print(f\"num of biterms: {len(encoded_biterms)}\")\n",
    "\n",
    "    # Train BTM on GPU\n",
    "    model = BTM_GPU(K=50, iterations=500)\n",
    "    model.fit(encoded_biterms, vocab_size=len(w2id), doc_biterms=doc_biterms)\n",
    "\n",
    "    # Print topics\n",
    "    topics = model.top_words(id2w, top_n=10)\n",
    "    for i, t in enumerate(topics):\n",
    "        print(f\"Topic {i}: {t}\")\n",
    "\n",
    "    # Inference example\n",
    "    print(\"\\nDocument 0 topic distribution:\")\n",
    "    print(model.infer_doc_topics(doc_biterms[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
